<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>Research Publications</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Human Behavior Modeling Lab">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <p class="lead"> University at Buffalo, SUNY </p>
            <h1>Human Behavior Modeling Lab</h1>
         </header>
         <div class="masthead">
            <div class="navbar">
               <div class="navbar-inner">
                  <div class="container">
                     <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="people.html">People</a></li>
                        <li class="active"><a href="#">Research</a></li>
                        <!-- <li><a href="publications.html">Publications</a></li> -->
                        <li><a href="teaching.html">Teaching</a></li>
                        <!-- <li><a href="gallery.html">Gallery</a></li> -->
                        <!-- <li><a href="news.html">News</a></li> -->
                        <li><a href="contact.html">Contact</a></li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>
         <!-- <hr> -->
         <div class="row-fluid">
            <div class="span12">
               <div class="page-header">
                  <h2>Latest Publications</h2>
                  <p>More Publication at this link: <a href="https://scholar.google.com/citations?user=pCOmTY0AAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a></p>
               </div>
               <!-- <h4>3D Human Behavior Generation and Interaction with Social Robots</h4> -->
               <ol style="font-size: 16px; line-height: 1.8;">
                  <li>
                    <b>AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot</b><br/>
                    X Wang*, L Dong*, S Rangasrinivasan, I Nwogu, S Setlur, V Govindaraju<br/>
                    arXiv preprint arXiv:2503.06791, 2025
                  </li>
                  <li>
                    <b>FUSE-MOS: Fusion of Speech Embeddings for MOS Prediction with Uncertainty Quantification</b><br/>
                    E Hoq, N Gupta, D Omondi, I Nwogu<br/>
                    Proc. Interspeech 2025, 2350-2354, 2025
                 </li>
                 <li>
                    <b>Exploring the Differences between Deaf and Hearing Infant Cries</b><br/>
                    E Hoq, I Nwogu<br/>
                    ICASSP 2025 – IEEE International Conference on Acoustics, Speech and …, 2025
                 </li>
                  <li>
                     <b>Body Talk: Thoughts and Feelings in Action</b><br/>
                     MG Frank, ZR Glowacki, M Neurohr, I Nwogu, A Solbu<br/>
                     Body Language Communication, 315-354, 2025
                  </li>
                  <li>
                    <b>Word-Conditioned 3D American Sign Language Motion Generation</b><br/>
                    L Dong, X Wang, I Nwogu<br/>
                    Association for Computational Linguistics, 2024
                 </li>
                  <li>
                    <b>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</b><br/>
                    L Dong, L Chaudhary, F Xu, X Wang, M Lary, I Nwogu<br/>
                    2024 IEEE 18th International Conference on Automatic Face and Gesture …, 2024
                 </li>
                  <li>
                     <b>Ig3d: Integrating 3D Face Representations in Facial Expression Inference</b><br/>
                     L Dong*, X Wang*, S Setlur, V Govindaraju, I Nwogu<br/>
                     European Conference on Computer Vision, 404-421, 2024
                  </li>
                  <li>
                     <b>Towards Open Domain Text-Driven Synthesis of Multi-person Motions</b><br/>
                     M Shan, L Dong, Y Han, Y Yao, T Liu, I Nwogu, GJ Qi, M Hill<br/>
                     European Conference on Computer Vision, 67-86, 2024
                  </li>
                  <li>
                    <b>A Comparative Study of Video-based Human Representations for American Sign Language Alphabet Generation</b><br/>
                    F Xu, L Chaudhary, L Dong, S Setlur, V Govindaraju, I Nwogu<br/>
                    2024 IEEE 18th International Conference on Automatic Face and Gesture …, 2024
                 </li>
                  <li>
                    <b>Cross-Attention Based Influence Model for Manual and Nonmanual Sign Language Analysis</b><br/>
                    L Chaudhary, F Xu, I Nwogu<br/>
                    International Conference on Pattern Recognition, 372-386, 2024
                 </li>
                  <li>
                    <b>Dataset Infant Anonymization with Pose and Emotion Retention</b><br/>
                    M Lary, M Klawonn, D Messinger, I Nwogu<br/>
                    2024 IEEE 18th International Conference on Automatic Face and Gesture …, 2024
                 </li>
                  <li>
                     <b>The Effect of Synchrony of Happiness on Facial Expression of Negative Emotion when Lying</b><br/>
                     A Solbu, MG Frank, F Xu, I Nwogu, M Neurohr<br/>
                     Journal of Nonverbal Behavior 48 (1), 73-92, 2024
                  </li>

                  <li>
                     <b>Language-guided Human Motion Synthesis with Atomic Actions</b><br/>
                     Y Zhai, M Huang, T Luan, L Dong, I Nwogu, S Lyu, D Doermann, J Yuan<br/>
                     Proceedings of the 31st ACM International Conference on Multimedia, 5262-5271, 2023
                  </li>
                  <li>
                     <b>AI-driven Sign Language Interpretation for Nigerian Children at Home</b><br/>
                     I Nwogu, R Peiris, K Dantu, R Gamta, E Asonye<br/>
                     International Joint Conferences on Artificial Intelligence, 2023
                  </li>
                  <li>
                     <b>Analyzing Interactions in Paired Egocentric Videos</b><br/>
                     A Khatri, Z Butler, I Nwogu<br/>
                     2023 IEEE 17th International Conference on Automatic Face and Gesture …, 2023
                  </li>
                  <li>
                     <b>SignNet: Single Channel Sign Generation Using Metric Embedded Learning</b><br/>
                     T Ananthanarayana, L Chaudhary, I Nwogu<br/>
                     2023 IEEE 17th International Conference on Automatic Face and Gesture …, 2023
                  </li>
                  <li>
                     <b>SignNet II: A Transformer-based Two-way Sign Language Translation Model</b><br/>
                     L Chaudhary, T Ananthanarayana, E Hoq, I Nwogu<br/>
                     IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (11 …), 2022
                  </li>
                  <li>
                     <b>Regression with Uncertainty Quantification in Large Scale Complex Data</b><br/>
                     N Wilkins, M Johnson, I Nwogu<br/>
                     2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC …), 2022
                  </li>
                  <li>
                     <b>Analyzing the Prosodic and Lingual Features of Popular Speakers</b><br/>
                     B Jethra, R Golhar, I Nwogu<br/>
                     International Conference on Pattern Recognition, 417-427, 2022
                  </li>
               </ol>
            </div>
         </div>
      </div>
      <footer id="footer">
         <div class="container-fluid">
            <div class="row-fluid">
               <div class="span5">
                  <h3>Contact Information</h3>
                  <p><b>Office Hours: </b>Monday-Friday (8.00am - 5.00pm)</p>
                  <p><b>Phone: </b>999-999-9999</p>
                  <p><b>Cell: </b>999-999-9999</p>
                  <a href="mailto:your.email@uni.edu">Email</a>
               </div>
               <div class="span2">
                  <a href="https://mybrandnewlogo.com/"><img src = "labimages/HBML_logo.png" alt="research-lab-logo"/></a>
               </div>
               <div class="span5">
                  <h3>Address</h3>
                  <p>Human Behavior Modeling Lab<br>
                     Phone:  (716) 645-1588<br>
                     305 Davis Hall<br>
                     Buffalo, NY, 14260<br>
                  </p>
                  <a href="http://maps.google.com/">Show Map</a>
               </div>
            </div>
         </div>
      </footer>

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
   </body>
   
</html>

